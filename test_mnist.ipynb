{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdiffeq import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "12.8\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())\n",
    "print(torch.version.cuda)  # Should print 12.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ODE function as before\n",
    "# This class defines a neural network module that represents the function for an Ordinary Differential Equation (ODE).\n",
    "# It is typically used in Neural ODEs (Neural Ordinary Differential Equations), where the forward pass involves solving an ODE.\n",
    "class ODEFunc(nn.Module):  # Inherits from PyTorch's nn.Module\n",
    "    def __init__(self, hidden_dim):\n",
    "        # Constructor for the ODEFunc class\n",
    "        # `hidden_dim` is the dimensionality of the input and output of the ODE function.\n",
    "        super(ODEFunc, self).__init__()  # Initializes the parent nn.Module class\n",
    "        \n",
    "        # Define a simple feedforward neural network with:\n",
    "        # - An input layer of size `hidden_dim`\n",
    "        # - A hidden layer with 50 units and ReLU activation\n",
    "        # - An output layer of size `hidden_dim`\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 50),  # Fully connected layer: input -> hidden\n",
    "            nn.ReLU(),                 # Activation function: ReLU\n",
    "            nn.Linear(50, hidden_dim)  # Fully connected layer: hidden -> output\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        # Forward pass of the ODE function\n",
    "        # `t` is the time variable (often unused in simple ODE functions like this)\n",
    "        # `x` is the input tensor\n",
    "        # Returns the output of the neural network applied to `x`\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ODE block that integrates the ODE function\n",
    "class ODEBlock(nn.Module):\n",
    "    def __init__(self, odefunc, t=torch.tensor([0., 1.])):\n",
    "        # Constructor for the ODEBlock class\n",
    "        # `odefunc` is an instance of the ODEFunc class (or any compatible ODE function)\n",
    "        # `t` is a tensor specifying the time interval for solving the ODE\n",
    "        super(ODEBlock, self).__init__()\n",
    "        self.odefunc = odefunc  # Store the ODE function\n",
    "        self.register_buffer('t', t)  # Register `t` as a buffer (non-trainable parameter)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass of the ODEBlock\n",
    "        # `x` is the input tensor (initial state for the ODE solver)\n",
    "        # Solves the ODE defined by `odefunc` over the time interval `t`\n",
    "        out = odeint(self.odefunc, x, self.t)  # `odeint` integrates the ODE\n",
    "        return out[-1]  # Return the final state (last time step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the full Neural ODE model for MNIST\n",
    "class NeuralODEMNIST(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        # Constructor for the NeuralODEMNIST class\n",
    "        # `hidden_dim` specifies the dimensionality of the hidden layer in the ODE block\n",
    "        super(NeuralODEMNIST, self).__init__()\n",
    "        \n",
    "        # Convolutional layers for feature extraction\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=1, padding=1),  # Convolution: 1 input channel -> 16 output channels\n",
    "            nn.ReLU(),                                # Activation function: ReLU\n",
    "            nn.MaxPool2d(2),                          # Downsampling: 2x2 max pooling\n",
    "            nn.Conv2d(16, 32, 3, stride=1, padding=1),# Convolution: 16 input channels -> 32 output channels\n",
    "            nn.ReLU(),                                # Activation function: ReLU\n",
    "            nn.MaxPool2d(2)                           # Downsampling: 2x2 max pooling\n",
    "        )\n",
    "        # After two pooling layers, the image size reduces from 28x28 to 7x7\n",
    "        \n",
    "        # Fully connected layer to map the flattened features to the hidden dimension\n",
    "        self.fc_in = nn.Linear(32 * 7 * 7, hidden_dim)\n",
    "        \n",
    "        # ODE block for modeling continuous dynamics\n",
    "        self.odeblock = ODEBlock(ODEFunc(hidden_dim))\n",
    "        \n",
    "        # Fully connected output layer to map the hidden dimension to 10 classes (digits 0-9)\n",
    "        self.fc_out = nn.Linear(hidden_dim, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass of the NeuralODEMNIST model\n",
    "        x = self.conv(x)            # Apply convolutional layers\n",
    "        x = x.view(x.size(0), -1)   # Flatten the output of the convolutional layers\n",
    "        x = self.fc_in(x)           # Map to the hidden dimension\n",
    "        x = self.odeblock(x)        # Apply the ODE block\n",
    "        x = self.fc_out(x)          # Map to the output classes\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training parameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "\n",
    "# Define transforms for the MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize using MNIST mean and std\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NeuralODEMNIST(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_in): Linear(in_features=1568, out_features=64, bias=True)\n",
      "  (odeblock): ODEBlock(\n",
      "    (odefunc): ODEFunc(\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=50, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=50, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Download and load the MNIST training and test datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = NeuralODEMNIST().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [0/938], Loss: 2.3122\n",
      "Epoch [1/5], Batch [100/938], Loss: 0.2021\n",
      "Epoch [1/5], Batch [200/938], Loss: 0.1393\n",
      "Epoch [1/5], Batch [300/938], Loss: 0.0466\n",
      "Epoch [1/5], Batch [400/938], Loss: 0.1049\n",
      "Epoch [1/5], Batch [500/938], Loss: 0.0866\n",
      "Epoch [1/5], Batch [600/938], Loss: 0.0269\n",
      "Epoch [1/5], Batch [700/938], Loss: 0.0547\n",
      "Epoch [1/5], Batch [800/938], Loss: 0.0582\n",
      "Epoch [1/5], Batch [900/938], Loss: 0.0454\n",
      "Epoch [1/5] Average Loss: 0.1459\n",
      "Epoch [2/5], Batch [0/938], Loss: 0.0392\n",
      "Epoch [2/5], Batch [100/938], Loss: 0.0288\n",
      "Epoch [2/5], Batch [200/938], Loss: 0.0717\n",
      "Epoch [2/5], Batch [300/938], Loss: 0.0079\n",
      "Epoch [2/5], Batch [400/938], Loss: 0.1161\n",
      "Epoch [2/5], Batch [500/938], Loss: 0.0725\n",
      "Epoch [2/5], Batch [600/938], Loss: 0.0475\n",
      "Epoch [2/5], Batch [700/938], Loss: 0.0198\n",
      "Epoch [2/5], Batch [800/938], Loss: 0.0123\n",
      "Epoch [2/5], Batch [900/938], Loss: 0.0637\n",
      "Epoch [2/5] Average Loss: 0.0498\n",
      "Epoch [3/5], Batch [0/938], Loss: 0.0095\n",
      "Epoch [3/5], Batch [100/938], Loss: 0.0420\n",
      "Epoch [3/5], Batch [200/938], Loss: 0.0110\n",
      "Epoch [3/5], Batch [300/938], Loss: 0.0082\n",
      "Epoch [3/5], Batch [400/938], Loss: 0.0584\n",
      "Epoch [3/5], Batch [500/938], Loss: 0.0116\n",
      "Epoch [3/5], Batch [600/938], Loss: 0.0276\n",
      "Epoch [3/5], Batch [700/938], Loss: 0.0474\n",
      "Epoch [3/5], Batch [800/938], Loss: 0.0052\n",
      "Epoch [3/5], Batch [900/938], Loss: 0.0152\n",
      "Epoch [3/5] Average Loss: 0.0366\n",
      "Epoch [4/5], Batch [0/938], Loss: 0.0257\n",
      "Epoch [4/5], Batch [100/938], Loss: 0.0392\n",
      "Epoch [4/5], Batch [200/938], Loss: 0.0222\n",
      "Epoch [4/5], Batch [300/938], Loss: 0.1239\n",
      "Epoch [4/5], Batch [400/938], Loss: 0.0178\n",
      "Epoch [4/5], Batch [500/938], Loss: 0.0407\n",
      "Epoch [4/5], Batch [600/938], Loss: 0.0479\n",
      "Epoch [4/5], Batch [700/938], Loss: 0.0029\n",
      "Epoch [4/5], Batch [800/938], Loss: 0.0448\n",
      "Epoch [4/5], Batch [900/938], Loss: 0.0115\n",
      "Epoch [4/5] Average Loss: 0.0286\n",
      "Epoch [5/5], Batch [0/938], Loss: 0.0021\n",
      "Epoch [5/5], Batch [100/938], Loss: 0.0078\n",
      "Epoch [5/5], Batch [200/938], Loss: 0.0007\n",
      "Epoch [5/5], Batch [300/938], Loss: 0.0066\n",
      "Epoch [5/5], Batch [400/938], Loss: 0.0016\n",
      "Epoch [5/5], Batch [500/938], Loss: 0.0228\n",
      "Epoch [5/5], Batch [600/938], Loss: 0.0139\n",
      "Epoch [5/5], Batch [700/938], Loss: 0.0334\n",
      "Epoch [5/5], Batch [800/938], Loss: 0.0031\n",
      "Epoch [5/5], Batch [900/938], Loss: 0.0590\n",
      "Epoch [5/5] Average Loss: 0.0225\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(1, epochs+1):\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)  # Move data and labels to the selected device (CPU/GPU)\n",
    "        optimizer.zero_grad()  # Clear the gradients from the previous step\n",
    "        output = model(data)  # Forward pass: compute model predictions\n",
    "        loss = criterion(output, target)  # Compute the loss\n",
    "        loss.backward()  # Backpropagation: compute gradients\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "        total_loss += loss.item()  # Accumulate the loss for the current batch\n",
    "        if batch_idx % 100 == 0:  # Print progress every 100 batches\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    avg_loss = total_loss / len(train_loader)  # Compute the average loss for the epoch\n",
    "    print(f\"Epoch [{epoch}/{epochs}] Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.91%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test dataset\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)  # Move data and labels to the selected device (CPU/GPU)\n",
    "        output = model(data)  # Forward pass: compute model predictions\n",
    "        # The class with the highest logit is the prediction\n",
    "        pred = output.argmax(dim=1)  # Get the index of the highest logit (predicted class)\n",
    "        correct += (pred == target).sum().item()  # Count correct predictions\n",
    "        total += target.size(0)  # Count total samples\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")  # Compute and print the accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
